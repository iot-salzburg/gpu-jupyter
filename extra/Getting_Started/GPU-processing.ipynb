{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Jupyter\n",
    "\n",
    "[GPU-Jupyter](https://github.com/iot-salzburg/gpu-jupyter): Your GPU-accelerated JupyterLab with PyTorch, TensorFlow, and a rich data science toolstack for your reproducible deep learning experiments.\n",
    "\n",
    "In this notebook, we test the installation of NVIDIA drivers, CUDA, and if PyTorch and Tensorflow can access the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 9.9.0\n",
      "ipykernel        : 7.1.0\n",
      "ipywidgets       : 8.1.5\n",
      "jupyter_client   : 8.7.0\n",
      "jupyter_core     : 5.9.1\n",
      "jupyter_server   : 2.15.0\n",
      "jupyterlab       : 4.5.1\n",
      "nbclient         : 0.10.4\n",
      "nbconvert        : 7.16.6\n",
      "nbformat         : 5.10.4\n",
      "notebook         : 7.5.1\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.14.3\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GPU Support\n",
    "\n",
    "Using `nvidia-smi`, the latest supported CUDA version is listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  9 07:57:09 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   48C    P8             54W /  300W |       1MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   40C    P8             25W /  300W |       1MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Tue_May_27_02:21:03_PDT_2025\n",
      "Cuda compilation tools, release 12.9, V12.9.86\n",
      "Build cuda_12.9.r12.9/compiler.36037853_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TensorFlow Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: '2.20.0'\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "tf.Tensor(389.54736, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767945432.665107    1118 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46731 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "I0000 00:00:1767945432.666721    1118 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46731 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # deactivate minor TF warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# set a seed for the experiment's repeatability, thus ACM's R1\n",
    "tf.random.set_seed(seed=23)\n",
    "\n",
    "print(f\"Tensorflow version: '{tf.__version__}'\")\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PyTorch Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: '2.9.1+cu128'\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4283, 0.2889, 0.4224],\n",
       "        [0.3571, 0.9577, 0.1100],\n",
       "        [0.2933, 0.9205, 0.5876],\n",
       "        [0.1299, 0.6729, 0.1028],\n",
       "        [0.7876, 0.5540, 0.4653]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# set a seed for the experiment's repeatability, thus ACM's R1\n",
    "RANDOM_SEED = 23\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# print the PyTorch version and if the GPU is accessible\n",
    "print(f\"PyTorch version: '{torch.__version__}'\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "a = torch.rand(5, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance test\n",
    "\n",
    "Now we want to know how much faster a typical operation is using a GPU. Therefore, we compute the same operation in NumPy, PyTorch (CPU), PyTorch (GPU), and TensorFlow (GPU). The operation is the calculation of the prediction matrix in linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10000, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 ms ± 11.7 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "H = x.dot(np.linalg.inv(x.transpose().dot(x))).dot(x.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10000, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.8 ms ± 3.13 ms per loop (mean ± std. dev. of 5 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 5\n",
    "# Calculate the projection matrix of x on the CPU\n",
    "H = x.mm( (x.t().mm(x)).inverse() ).mm(x.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) PyTorch on GPU via CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3186, 0.6480, 0.0896, 0.6143, 0.8915],\n",
      "        [0.5869, 0.9017, 0.4452, 0.4422, 0.7567],\n",
      "        [0.2252, 0.6995, 0.4897, 0.3269, 0.5074],\n",
      "        [0.8776, 0.7383, 0.6485, 0.0821, 0.6425],\n",
      "        [0.9088, 0.3180, 0.8072, 0.1974, 0.5014]], device='cuda:0')\n",
      "tensor([[0.3186, 0.6480, 0.0896, 0.6143, 0.8915],\n",
      "        [0.5869, 0.9017, 0.4452, 0.4422, 0.7567],\n",
      "        [0.2252, 0.6995, 0.4897, 0.3269, 0.5074],\n",
      "        [0.8776, 0.7383, 0.6485, 0.0821, 0.6425],\n",
      "        [0.9088, 0.3180, 0.8072, 0.1974, 0.5014]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    x = torch.rand(10000, 256, device=device) # directly create a tensor on GPU\n",
    "    y = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    print(x[0:5, 0:5])\n",
    "    print(y.to(\"cpu\", torch.double)[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.63 ms ± 3.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10 -r 7\n",
    "# Calculate the projection matrix of x on the GPU\n",
    "H = x.mm( (x.t().mm(x)).inverse() ).mm(x.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TensorFlow on GPU via CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9438342  0.3385067  0.7677115  0.6680695  0.3649447 ]\n",
      " [0.27590752 0.59973633 0.60091686 0.69706666 0.6652031 ]\n",
      " [0.05486917 0.60015345 0.7303884  0.86384666 0.1850121 ]\n",
      " [0.25567186 0.04193914 0.43598843 0.30942154 0.5328213 ]\n",
      " [0.07237387 0.17294586 0.9945524  0.182423   0.22695827]]\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    # Create a tensor on GPU (TF will place it on GPU by default when available)\n",
    "    x = tf.random.uniform([10000, 256], dtype=tf.float32)\n",
    "    y = tf.identity(x)  # equivalent to \"move/copy\" (will stay on GPU if x is on GPU)\n",
    "    # Print slices\n",
    "    print(x[:5, :5].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.89 ms ± 303 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1767945437.550987    1118 cuda_solvers.cc:175] Creating GpuSolver handles for stream 0x5c5bb8bcbfa0\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10 -r 7\n",
    "# Projection matrix: H = x (x^T x)^{-1} x^T\n",
    "H = tf.matmul(x, tf.linalg.solve(tf.matmul(tf.transpose(x), x), tf.transpose(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
